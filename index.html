<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	
	<head>
		<title>NIHARIKA'S PORTFOLIO</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<style>
			/* Remove list styling */
			nav ul {
			  list-style: none;
			  padding: 0;
			}
		
			/* Style anchor links */
			nav a {
			  text-decoration: none;
			  color: #333; /* Change the color as needed */
			}
		
			/* Add bullet points and center them */
			ul {
			  list-style-position: inside;
			  text-align: center;
			}
			#main h2 {
      font-size: 1.5em; /* Adjust the value as needed */
    }

    #main ul {
      font-size: 1.2em; /* Adjust the value as needed */
    }

    /* Increase font size for the table of contents items */
    #main ul li {
      font-size: 1.2em; /* Adjust the value as needed */
    }
		  </style>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Intro -->
					<div id="intro">
						<h1>Niharika Kolliboyana<br />
						</h1>
						<p>Data Analyst skilled in Analytical Tools with a Dash of Machine Learning</p>
						<ul class="actions">
							<li><a href="#header" class="button icon solid solo fa-arrow-down scrolly">Continue</a></li>
						</ul>
					</div>

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">Let's Jump!</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="active"><a href="index.html">MY PORTFOLIO</a></li>
			
						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/niharika1307k/" class="icon brands fa-linkedin"><span class="label">linkedin</span></a></li>
				
							<li><a href="https://github.com/Niharika1307" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>
					
				<div id="main">
				
						<!-- Featured Post -->
							<article class="post featured">
								<header class="major">
						<h2>Table of Contents</h2>
					<ul>
					      <li><a href="#section1">About Me</a></li>
						  <li><a href="#section2">Skills</a></li>
						  <li><a href="#section3">Experience</a></li>
						  <li><a href="#section4">Projects</a></li>
						  <li><a href="#section5">Research</a></li>
						  <li><a href="#section6">Case Study</a></li>
						  <li><a href="#section7">Education</a></li>
						  <li><a href="#section8">Certifications</a></li>
						  <!-- Add more sections as needed -->
					</ul>
					<hr>
					<div id="main">

						<!-- Featured Post -->
						
							<article class="post featured">
								<header class="major">
				
							        <h2>ABOUT  ME<br />
									</a></h2>
									<p>
										I wholeheartedly embrace the philosophy that 'Data will talk to you if you're willing to listen,' as wisely stated by Jim Bergeson. My journey began with a bachelor's in computer science, where I discovered a passion for courses rich in data exploration. Transitioning into data analysis, I navigated diverse industries, from healthcare to finance, honing practical expertise. Pursuing a master's in business analytics deepened my understanding, fusing theoretical and practical knowledge. As an AI/ML Engineer Intern, I delved into machine learning, uncovering a penchant for predictive analytics and creative problem-solving. This journey fueled extensive research and personal projects, utilizing real-time and publicly available data. From intensive case studies to theoretical exploration, my narrative is a testament to my insatiable curiosity and commitment to leveraging data for meaningful insights. Ready to contribute, my story invites exploration into the realms of analytics, machine learning, and the limitless possibilities of data. </p>
								</header>
								<a class="image main"><img src="images/me3.jpg" alt="" /></a>
								
							</article>
							<div id="skills">

								<!-- Featured Post -->
									<article class="post featured">
										<header class="major">
								 <h2>Skills</h2>
								<table class="table-style">
								  <tr>
									<td>SQL</td>
									<td>⭐⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>Python</td>
									<td>⭐⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>MS Excel</td>
									<td>⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>Power BI</td>
									<td>⭐⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>Tableau</td>
									<td>⭐⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>Machine Learning</td>
									<td>⭐⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>JIRA</td>
									<td>⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>Oracle</td>
									<td>⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>Microsost Office Suite</td>
									<td>⭐⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>R</td>
									<td>⭐⭐⭐</td>
								  </tr>
								<tr>
									<td>Six Sigma</td>
									<td>⭐⭐⭐⭐⭐</td>
								  </tr>
								</table>
							  </div>
					<style>
						/* Your existing styles here */
					
						/* Style the experience section */
						#experience {
						  margin-top: 2em;
						}
					
						#experience h2 {
						  font-size: 1.5em;
						  margin-bottom: 0.5em;
						}
					
						#experience .job {
						  margin-bottom: 1.5em;
						}
					
						#experience .job h3 {
						  font-size: 1.2em;
						  margin-bottom: 0.5em;
						}
					
						#experience .job p {
						  font-size: 1em;
						  margin-bottom: 0.5em;
						}
					  </style>
					</head>
					<body class="is-preload">
					
					<div id="experience">

						<!-- Featured Post -->
							<article class="post featured">
								<header class="major">
					
					 
							<h2>Experience</h2>
						  </header>

						  <div class="job">
							<h3>AI/ML Engineer Intern</h3>
							<p>Ameriinfo Vets</p>
							<p>Jan 2023 - Aug 2023</p>
							<p>I spearheaded the automation of PPE recognition in industrial settings by deploying YOLOv5 and integrating it with the DOTMLPF-P framework, aligning the technology with safety protocols and achieving a 25% reduction in safety violations. Additionally, I implemented a streamlined Data Conversion Lifecycle for PPE recognition, leveraging Linux shell scripts for efficient preprocessing of a 50,000-image dataset, contributing to a 25% reduction in safety violations and enhancing system documentation. My efforts in documenting the System Lifecycle through PyTorch model fine-tuning resulted in a 15% precision increase and an impressive 80% decrease in workplace accidents, demonstrating a comprehensive approach to safety and efficiency.</p>
						  </div>
					
						  <!-- Job 1 -->
						  <div class="job">
							<h3>Data Analyst</h3>
							<p>Aditya Birla Sun Life Insurance</p>
							<p>Jun 2020 - May 2022</p>
							<p>I excelled in conducting ad-hoc analyses, optimizing infrastructure costs, and implementing sophisticated fraud detection algorithms within the insurance domain. Leveraging SQL, Ms Excel, and SAS, I provided actionable insights, reduced fraudulent claim payouts, and achieved a significant $3 million reduction in infrastructure costs through MySQL database migration to the cloud. My expertise includes tracking insurance KPIs in Power BI, implementing ML workflows on DataBricks, and accelerating decision-making processes for actuarial teams by 27%. My commitment to efficiency is evident in the optimization of data storage and retrieval processes, resulting in a 40% improvement in scalability and performance through Snowflake Data Cloud.</p>
						  </div>
					
						  <!-- Job 2 (Repeat structure for each job) -->
						  <div class="job">
							<h3>Healthcare Data Analyst</h3>
							<p>King George Hospital</p>
							<p>Jan 2018 - May 2020</p>
							<p>I executed Python and SQL-based data analysis, leading to improved hospital resource utilization. I implemented key performance indicators in Tableau, automated reporting in Power BI, and enhanced Excel reports, reducing data discrepancies and ensuring accuracy. My role involved conducting ad-hoc analyses in Python, contributing to agile responses in healthcare sales, and developing automated data pipelines for reporting efficiency.</p>
						  </div>
					
						  <!-- Repeat for other jobs -->
					
						</article>
					  </div>
			
				</div>
				<hr>
				<!-- Main -->
					

						<!-- Posts -->
						<article class="projects">
							<header class="major">
								
										<h1>Projects<br />
										</a></h1>
					
									
									<p>I've successfully navigated diverse industries, showcasing a broad spectrum of data analytics skills. From telecom and energy to finance and online marketplaces like Airbnb, I've gathered, cleaned, and analyzed extensive datasets. Utilizing tools such as Ms Excel, Python, SQL,Power BI and Tableau, I've demonstrated proficiency in data cleaning, statistical modeling, and feature engineering. My predictive modeling expertise shines through projects such as telecom customer churn analysis, US power generation insights, personal loan acceptance prediction, loan default and loss severity assessment, image classification using ConvNets, sentiment analysis on IMDb reviews, and crafting targeted insights dashboards in Power BI and the addition of navigating Airbnb prosperity exemplifies my ability to extract valuable insights from historical data using Tableau, further emphasizing my diverse and robust skill set as a data analyst.</p>
									<ul class="actions special">
										
									</ul>
									</header>
						<article>
							<header>
								
										<h3>Customer Churn at a Telecom Company<br />
										</a></h3>
					                </header>
									<a class="image fit"><img src="images/churn.jpg",width="300" 
										height="350"  alt="" /></a>
									<p>This project seamlessly integrated various tools and techniques to tackle the complex issue of predicting customer churn in the telecom industry. Utilizing SQL for data manipulation and Excel for efficient data storage, we ensured a structured and accessible dataset. Python played a pivotal role in the exploratory data analysis (EDA) phase, allowing us to uncover patterns and trends within the data. Additionally, Decision Tree was employed for predictive modeling, creating a robust system capable of accurately forecasting customer churn. This multi-faceted approach, combining SQL, Excel, and Python, not only facilitated a comprehensive understanding of the dataset but also enabled the development of a powerful predictive model, contributing to successful outcomes in mitigating customer churn.</p>
									<ul class="actions special">
										<li><a href="https://github.com/Niharika1307/Churn-Prediction/blob/main/churn/Churn_Decision_Tree.ipynb" class="button">View Project</a></li>
									</ul>
									
								</article>
								
								<article>
									<header>
							
										<h3>US Power Generation: K-Means Insights from PUDL Dataset<br />
										</a></h3>
									</header>
									<a class="image fit"><img src="images/FUEL.jpg" alt="" /></a>
									<p>This analysis delves into the Public Utility Data Preparation (PUDL) dataset, employing the K-Means clustering algorithm to efficiently segment vast data for insights into US power generation. By determining optimal clustering (k=3) through the WSS and Silhouette methods, the study identifies three distinct clusters. Notably, while Cluster 1, dominated by coal, is cost-efficient, it carries environmental concerns. In contrast, Cluster 2, featuring natural gas, emerges as a promising choice with reasonable costs and minimal impurities. The findings advocate for Natural Gas as an environmentally friendly and cost-effective fuel for US power generation. The K-Means approach proves invaluable for its speed, making it a practical choice for clustering extensive datasets.</p>
									<ul class="actions special">
										<li><a href="https://github.com/Niharika1307/64060_-nkollibo/blob/main/FINAL%20PROJECT/Fuel_cost%20(1).ipynb" class="button">View Project</a></li>
									</ul>
								</article>
								<article>
									<header>
										
										<h3>Predicting Personal Loan Acceptance: Naive Bayes Analysis on Universal Bank Data<br />
										</a></h3>
									</header>
									<a class="image fit"><img src="images/LOAN.jpg" alt="" /></a>
									<p>Conducting a comprehensive analysis on 5000 Universal Bank customers, this study utilized Python, along with a diverse set of tools including EDA, SQL, Excel, and statistical analysis. Focusing on predictors like Online and Credit Card, and employing Naive Bayes, the analysis successfully predicted personal loan acceptance. Out of the sampled customers, 9.6% accepted the offered personal loan. The model, enriched by demographic information and the customer's bank relationship, achieved an impressive accuracy of 90.35%. This holistic approach involving diverse tools showcases the multifaceted nature of the analysis, ensuring robust insights for targeted marketing strategies and informed decision-making in personal loan campaigns.</p>
									<ul class="actions special">
										<li><a href="https://github.com/Niharika1307/64060_-nkollibo/blob/main/Assignment_3/Naive_Bayes.ipynb" class="button">View Project</a></li>
									</ul>
								</article>
								<article>
									<header>
										
										<h3>Predicting Loan Default and Loss Severity using Random Forest Classifier<br />
										</a></h3>
									</header>
									<a class="image fit"><img src="images/Loss.jpg" alt="" /></a>
									<p>This project challenges the conventional approach to loan default prediction by not only distinguishing between good and bad counterparties but also anticipating and incorporating the severity of potential losses. Shifting from the binary classification of traditional banking, this endeavor bridges the gap between minimizing economic capital consumption and optimizing risk for financial investors. The evaluation metric for this competition is the mean absolute error (MAE). The project encompasses data exploration analysis, utilizing a modeling strategy grounded in Logistic Regression, and further enhancing the predictive capability with the application of a Random Forest Classifier. The estimation of model performance, guided by the MAE metric, provides a holistic perspective on anticipating loan defaults and assessing the associated loss severity, offering valuable insights for risk optimization in asset management.</p>
									<ul class="actions special">
										<li><a href="https://github.com/Niharika1307/BankLoanLossSeverity/blob/main/Project/ADM_FINAL%20(2).ipynb" class="button">View Project</a></li>
									</ul>
								</article>
								<article>
									<header>
										
										<h3>Enhancing Image Classification: Convnets for Cats & Dogs Using Deep Learning Techniques<br />
										</a></h3>
									</header>
									<a class="image fit"><img src="images/catsdog" alt="" /></a>
									<p>The project focuses on leveraging deep learning for computer vision tasks, specifically classifying images in the "Dog-vs-Cats" subset. Despite the challenge posed by limited data, a convolutional neural network (convnet) is developed from scratch, emphasizing its ability to learn spatial patterns in images. The dataset is meticulously prepared, comprising subsets for training, validation, and testing. Techniques such as preprocessing, data augmentation, and incorporating a pre-trained model (VGG16) are employed to enhance model performance. The results indicate that pre-trained models consistently outperform those trained from scratch, especially when dealing with restricted training data. Notably, increasing the training set size and adjusting validation set sizes contribute to improved model accuracy. Overall, the project underscores the effectiveness of deep learning techniques in image classification, even with limited datasets, offering insights into optimal model architectures and training strategies.
									</p>
									<ul class="actions special">
										<li><a href="https://github.com/Niharika1307/64061_-nkollibo/blob/main/Assignment-2/Cats_vs_Dogs.ipynb" class="button">View Project</a></li>
									</ul>
								</article>
								<article>
									<header>
										
										<h3>Sentiment Analysis on IMDB Reviews: Custom vs. Pre-trained Embedding Layers<br />
										</a></h3>
									</header>
									<a class="image fit"><img src="images/imdb.jpg",width="200",heights="100" alt="" /></a>
									<p>In this sentiment analysis study on IMDB movie reviews, we explored two embedding techniques: a custom-trained embedding layer and a pretrained word embedding layer using GloVe. Our dataset comprised 50,000 reviews, truncated after 150 words, with training samples ranging from 100 to 10,000. The custom-trained embedding layer showed high accuracy (97.5% to 98.5%) with optimal performance at a training sample size of 1000. Meanwhile, the pretrained GloVe model achieved accuracy ranging from 92.9% to 100%, peaking at a sample size of 100 but showing signs of overfitting with larger samples. Overall, the custom-trained embedding layer demonstrated superior performance, especially with larger training sets.
									</p>
									<ul class="actions special">
										<li><a href="https://github.com/Niharika1307/64061_-nkollibo/blob/main/Assignment-3/IMDB_EMBEDDING%20(1).ipynb" class="button">View Project</a></li>
									</ul>
								</article>
								<article>
									<header>
										
										<h3>Empowering the Portage APL with Targeted Insights:Dashboard in Power BI<br />
										</a></h3>
									</header>
									<a class="image fit"><img src="images/APL.jpg" alt=""/></a>
									<p>In a proactive initiative, delved into a real-time dataset to enhance strategic decision-making.Leveraging Tableau and Power BI,crafted a predictive dashboard,utilizing Data Analysis Expressions (DAX) measures and SQL for data manipulation.Excel played a pivotal role in Exploratory Data Analysis (EDA).
										The dashboard provides a holistic view, guiding resource allocation and shedding light on priority animal categories.The project explored correlations such as age impacting adoptions/euthanizations, city-wise adoption and euthanization trends,and year-wise analyses.Recommendations encompassed implementing background checks, event data collection, tracking expenses per animal, and documenting donation dates. This multifaceted approach equips Portage APL with actionable insights for the welfare of animals, employing advanced analytics techniques.
									</p>
						
									<ul class="actions special">
										<li><a href="https://app.powerbi.com/groups/me/reports/65460920-7b17-4ab3-9b79-e2c3a7ffcabd/ReportSection?experience=power-bi" class="button">View Project</a></li>
									</ul>
								</article>
								<article>
									<header>
										
										<h3>Navigating Airbnb Prosperity: Insights from Historical Data Unveiled with Tableau<br />
										</a></h3>
									</header>
									<a class="image fit"><img src="images/Dashboard 1.png" alt="" /></a>
									<p>Empowering aspiring entrepreneurs in the Airbnb realm, this meticulously crafted Tableau dashboard, driven by SQL-refined datasets, enables strategic decision-making. Prospective hosts gain a comprehensive view of demand dynamics, allowing precise decisions on optimal city locations, preferred property types, and specific room configurations. Moreover, the tool delves into seasonal patterns, ensuring hosts can strategically lease their properties to meet fluctuating demand throughout the year. This comprehensive approach provides entrepreneurs with actionable insights to make informed choices, maximizing their success in the Airbnb business.
									</p>
									<ul class="actions special">
										<li><a href="https://public.tableau.com/app/profile/niharika.kolliboyana2259/viz/shared/QNDWYTR47" class="button">View Project</a></li>
									</ul>
								</article>
								
					</article>
					<hr>
								<!-- Posts -->
						<article class="research">
							<header class="major">
								
										<h2>Research<br />
										</a></h2>
					
									
									<p>My research endeavors reflect a profound motivation to address contemporary challenges. The publication on "Improved Bot Identification with Imbalanced Data Using GG-XGBoost" demonstrates my commitment to countering the impact of bots on online social networks. The project on "Rail-Road Worker Safety Enhancement Through Computer Vision-Based Detection of Safety Helmets and Vests Using YOLOv5" reflects my passion for leveraging advanced computer vision techniques to revolutionize safety in the railroad industry. Grounded in diverse datasets and meticulous evaluations, these endeavors showcase my dedication to providing meaningful solutions in the realms of cybersecurity and occupational safety.</p>
									<ul class="actions special">
										
									</ul>
									</header>
						
							<article>
								<header>
									
									<h3>An Improved Bot Identification with Imbalanced Data using GG-XGBoost<br />
									</a></h3>
								</header>
								<a class="image fit"><img src="images/IEEE.1.jpg" alt="" /></a>
								<p>This work has been published in the proceedings of the 2022 2nd International Conference on Intelligent Technologies (CONIT).In the realm of online social networks (OSNs), the proliferation of bots disseminating fraudulent information poses a significant challenge. Despite being outnumbered by benign users, bots have a disproportionately negative impact. Conventional bot identification methods, reliant on supervised learning, often grapple with imbalanced datasets featuring fewer bot instances. This work introduces a novel approach, leveraging a generative adversarial network (GAN) with gated recurrent unit (GRU), to address the unbalanced bot distribution in OSNs. Additionally, a pioneering algorithm, GG-XGBoost, is proposed, seamlessly integrating GRU-GAN with the XGBoost model. Experimental validation on Twitter datasets underscores the efficacy of the GG-XGBoost algorithm in enhancing bot identification accuracy.
								</p>
								<ul class="actions special">
									<li><a href="https://github.com/Niharika1307/DMSB-VAE-GAN-" class="button">View Research</a></li>
								</ul>
							</article>
						
								<article>
									<header>
										
										<h3>Rail-Road Worker Safety Enhancement through 
											Computer Vision-Based Detection of Safety Helmets and 
											Vests using YOLOv5 <br />
										</a></h3>
									</header>
									<a class="image fit"><img src="images/rail.jpg" alt="" /></a>
									<p>This research focuses on enhancing rail-road worker safety through computer vision-based detection of safety helmets and vests using YOLOv5. The study addresses the challenge of monitoring personal protective equipment (PPE) compliance, crucial for minimizing workplace accidents. Traditional methods for PPE monitoring are manual, time-consuming, and prone to errors. The proposed solution involves a deep learning approach using YOLOv5, which proves effective in detecting safety helmets and vests worn by rail-road workers.

										The dataset used for training comprises images from various industrial settings, ensuring a diverse and representative set. The YOLOv5 architecture, pretrained on the COCO dataset, is employed for object detection. The model is fine-tuned and evaluated on precision and recall metrics. The results show high precision and recall values for safety helmets and vests, indicating the model's efficacy in identifying PPE.
										
										The study emphasizes the potential of YOLOv5 as a powerful tool for improving safety in the railroad industry. The automated PPE compliance monitoring system can contribute to a safer working environment by ensuring workers wear the required safety gear. The research also acknowledges challenges such as GDPR compliance and hardware requirements. Overall, the proposed approach presents a promising solution to enhance rail-road worker safety through advanced computer vision techniques.
									</p>
									<ul class="actions special">
										<li><a href="https://github.com/Niharika1307/64061_-nkollibo/blob/main/PAPER/Final_IEEE.pdf" class="button">View Research</a></li>
									</ul>
								</article>
					</article>
					<hr>
					<article class="case study">
						<header class="major">
							
									<h2>Case Study<br />
									</a></h2>
				
								
								<p>Exploring six diverse analytics case studies reveals crucial lessons, emphasizing the importance of meticulous planning, transparent communication, and continuous improvement across various industries.</p>
								<ul class="actions special">
									
								</ul>
								</header>
					
						<article>
							<header>
								
								<h3>Insights and Imperfections: A Deep Dive into Six Diverse Analytics Case Studies<br />
								</a></h2>
							</header>
							<a href="#" class="image fit"><img src="images/cs.jpg" alt="" /></a>
							<p>In examining six distinct case studies, various challenges and lessons in analytics projects emerge. The Boeing 737 MAX incidents underscore the critical need for robust risk management, transparent communication, and vigilant testing in safety-critical domains. The banking fraud detection case emphasizes the significance of considering performance requirements and conducting real-world testing. Amazon Rekognition's misidentification issues highlight the importance of diverse training data and transparency. IBM Watson's healthcare misstep emphasizes the necessity of thorough planning, unbiased data, and collaboration. The failure of AI for university admission suggests the importance of domain expertise and quality data. The loss of the Mars Orbiter stresses the need for effective communication, standardized measurement units, and adequate resource allocation. Collectively, these cases underscore the imperative for meticulous planning, diverse testing, transparent communication, and continuous improvement in analytics projects across various industries.
							</p>
							<ul class="actions special">
								<li><a href="https://github.com/Niharika1307/AIP-/tree/main/Analytical%20Case%20Study" class="button">View Case Study</a></li>
							</ul>
						</article>
							</section>
							<hr>
							<div id="main">

								<!-- Featured Post -->
									<article class="education">
										<header class="major">
							<h1>Education</h1>
							  <table class="table-style">
								<tr>
								  <td>Master's in Business Analytics at Kent State University</td>
								  <td>2022-23</td>
								</tr>
								<tr>
								  <td>Bachelor's in Computer Science & Engineering at GVPCEW</td>
								  <td>2018-22</td>
								</tr>
							  </table>
							</div>
				
							<hr>
							<div id="main">

								<!-- Featured Post -->
									<article class="certification">
										<header class="major">
							<h1>Certifications</h1>
							  <table class="table-style">
								<tr>
								  <td>Google Data Analytics</td>
								  <td>Coursera</td>
								</tr>
								<tr>
									<td>
										HackerRank SQL Assessment</td>
									<td>HackerRank</td>
								  </tr>
								  
								  <tr>
									<td>Agile Scrum Certifications</td>
									<td>SCRUMstudy</td>
								  </tr>
								<tr>
								  <td>
									Six Sigma Yellow Belt</td>
								  <td>VMEdu Inc.</td>
								</tr>
								<tr>
									<td>Python for Machine Learning & Data Science Masterclass</td>
									<td>Udemy</td>
								  </tr>
								
							  </table>
							</div>
                        
						<!-- Footer -->
							

					</div>

				<!-- Footer -->
					<footer id="footer">
						
						<section class="split contact">
							<section class="alt">
								<h3>Address</h3>
								<p>Twinsburg,OH 44087<br />
								</p>
							</section>
							
							<section>
								<h3>Email</h3>
								<p><a href="#">niharikakolliboyana@gmail.com</a></p>
							</section>
							
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a href="https://www.linkedin.com/in/niharika1307k/" class="icon brands fa-linkedin"><span class="label">linkedin</span></a></li>
				                    <li><a href="https://github.com/Niharika1307" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</section>
						</section>
					</footer>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Copyright</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>